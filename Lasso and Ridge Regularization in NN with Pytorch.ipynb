{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73917e0",
   "metadata": {},
   "source": [
    "optim is a submodule within the PyTorch library. It contains various optimization algorithms that are commonly used for training machine learning models, especially neural networks. These optimization algorithms are used to adjust the model's parameters (e.g., weights and biases) during training to minimize the loss function and improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adcc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66889ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Generate some random data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(1000, 5)  # 1000 samples, 5 features\n",
    "X[:, 0]+=1\n",
    "X[:, 1]+=2\n",
    "X[:, 2]+=3\n",
    "y = 2 * X[:, 0] + 7 * X[:, 1] + 10 * X[:, 2] + np.random.randn(1000)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17832726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.979512314250047\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c888df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c19b5",
   "metadata": {},
   "source": [
    "In the constructor (__init__ method) of the LassoRegression class, you pass two parameters:\n",
    "\n",
    "input_size: This parameter represents the number of features (input dimensions) for your linear regression model.\n",
    "l1_strength: This parameter represents the strength of L1 regularization, which controls how much the L1 penalty affects the model during training.\n",
    "Inside the constructor, super(LassoRegression, self).__init__() is used to call the constructor of the parent class (nn.Module). It's important to call the parent class constructor to properly initialize the LassoRegression class as a PyTorch module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712cbfa",
   "metadata": {},
   "source": [
    "In Python, when you define a subclass and you want to call the constructor of the parent (base) class, you typically use the super() function. The purpose of calling the parent class constructor with super() is to initialize the inherited attributes and behaviors defined in the parent class before adding any additional attributes specific to the subclass.\n",
    "\n",
    "The syntax you provided, super(LassoRegression, self).__init__(), is a way to call the constructor of the parent class (nn.Module) from within the LassoRegression class constructor. Let me explain the components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a77a0",
   "metadata": {},
   "source": [
    "The super() function returns a temporary object of the superclass, which allows you to call its methods. In this case, it returns an object that represents the nn.Module superclass.\n",
    "\n",
    "This is the current class in which you are defining the constructor (i.e., the subclass). super(LassoRegression, self) specifies that you want to call the constructor of the parent class (nn.Module) in the context of the LassoRegression class.\n",
    "\n",
    "self refers to the instance of the class itself, in this case, an instance of LassoRegression. When you call super(LassoRegression, self).__init__(), you are invoking the constructor of the parent class (nn.Module) and passing the current instance self as the first argument to that constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e0533",
   "metadata": {},
   "source": [
    "The purpose of passing self as the first argument to the parent class constructor is to ensure that the initialization of the parent class is done in the context of the current instance of LassoRegression. This is important because the parent class constructor may need to set up instance-specific attributes and behaviors.\n",
    "\n",
    "The line super(LassoRegression, self).__init__() is a standard practice in Python when creating subclasses. It ensures that the constructor of the parent class is called properly to initialize inherited attributes and behaviors while allowing you to customize and extend the subclass as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ad5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model with L1 regularization (Lasso)\n",
    "class LassoRegression(nn.Module):\n",
    "    def __init__(self, input_size, l1_strength):\n",
    "        super(LassoRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1, bias=True)\n",
    "        self.l1_strength = l1_strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0975fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with L1 regularization\n",
    "model = LassoRegression(input_size=5, l1_strength=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a2397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (mean squared error) with L1 regularization\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5506d1b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsi224\\.conda\\envs\\test_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # L1 regularization term\n",
    "    l1_reg = torch.abs(model.linear.weight).sum() * model.l1_strength\n",
    "    \n",
    "    # Add L1 regularization to the loss\n",
    "    total_loss = loss + l1_reg\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afc9d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Coefficients (with L1 regularization):\n",
      " [[ 4.6241956e+00  5.8891597e+00  5.8989525e+00 -3.9402531e-03\n",
      "   1.3515234e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned coefficients (weights)\n",
    "learned_coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Learned Coefficients (with L1 regularization):\\n\", learned_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff1e0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model with Ridge regularization\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_size, l2_strength):\n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1,bias = True)\n",
    "        self.l2_strength = l2_strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a1ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with Ridge regularization\n",
    "model = RidgeRegression(input_size=5, l2_strength=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e2b52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (mean squared error) with Ridge regularization\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb8c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # L2 regularization term\n",
    "    l2_reg = (model.linear.weight ** 2).sum() * model.l2_strength\n",
    "    \n",
    "    # Add L2 regularization to the loss\n",
    "    total_loss = loss + l2_reg\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7135ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Coefficients (with Ridge regularization):\n",
      " [[ 4.4364114   5.8936653   5.3988247  -0.17205386  0.06646103]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned coefficients (weights)\n",
    "learned_coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Learned Coefficients (with Ridge regularization):\\n\", learned_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e558d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model with Elastic Net regularization\n",
    "class ElasticNetRegression(nn.Module):\n",
    "    def __init__(self, input_size, l1_strength, l2_strength):\n",
    "        super(ElasticNetRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1, bias = True)\n",
    "        self.l1_strength = l1_strength\n",
    "        self.l2_strength = l2_strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab39e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with Elastic Net regularization\n",
    "model = ElasticNetRegression(input_size=5, l1_strength=1.0, l2_strength=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df890729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (mean squared error) with Elastic Net regularization\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1119d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # L1 regularization term\n",
    "    l1_reg = torch.abs(model.linear.weight).sum() * model.l1_strength\n",
    "    \n",
    "    # L2 regularization term\n",
    "    l2_reg = (model.linear.weight ** 2).sum() * model.l2_strength\n",
    "    \n",
    "    # Add Elastic Net regularization to the loss\n",
    "    total_loss = loss + l1_reg + l2_reg\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9d667f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Coefficients (with Elastic Net regularization):\n",
      " [[ 3.9642704e+00  5.5403581e+00  5.7475700e+00 -1.7321273e-03\n",
      "   3.5503949e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned coefficients (weights)\n",
    "learned_coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Learned Coefficients (with Elastic Net regularization):\\n\", learned_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b28909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fdbfc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LinearRegression(input_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "559e84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (mean squared error)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ebb53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14857f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Coefficients (Linear Regression without regularization):\n",
      " [[ 0.0780985   0.23204686  0.2930379  -0.00574475  0.00487926]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned coefficients (weights)\n",
    "learned_coefficients = model.linear.weight.detach().numpy()\n",
    "print(\"Learned Coefficients (Linear Regression without regularization):\\n\", learned_coefficients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
